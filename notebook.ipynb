{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "98iXpM6Hv_uA",
        "oL4Is76diAkK",
        "FgujKy-3iF0p",
        "k50yLlhdvijr",
        "vYk9wK8pvnYh",
        "Og5YkJ7wvrel",
        "Oagb-5H-veXO",
        "vG_wYWGIvxxq",
        "41CGLLSsv1Pd"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noro129/Internship_task_allInOnePlatform-Video_generation_service/blob/master/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import statements and global variables assignement"
      ],
      "metadata": {
        "id": "98iXpM6Hv_uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import random,os,subprocess\n",
        "from moviepy.editor import VideoFileClip, concatenate_videoclips"
      ],
      "metadata": {
        "id": "AU_UYmXYhzxr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ideas_file='file.txt'\n",
        "with open(ideas_file,'r') as file:\n",
        "  ideas=file.readlines()\n",
        "c=0\n",
        "for i in range(len(ideas)):\n",
        "  ideas[i]=ideas[i][0:len(ideas[i])-1]\n",
        "  c+=len(ideas[i])\n",
        "print(ideas)\n",
        "print(c)"
      ],
      "metadata": {
        "id": "iQo5flaNV2JN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86b578fe-aefb-4626-f072-6def03d49181"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fondée en 1933,', 'contenant 260 milliards de barils', 'devant les autres société']\n",
            "73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = [\"image1.jpeg\", \"image2.jpg\", \"image3.jpg\", \"image4.jpeg\"]\n",
        "fps=24\n",
        "total_duration = 0.5*c+0.5\n",
        "output_path = \"video.mp4\"\n",
        "final_video=\"final.mp4\"\n",
        "print(total_duration,' in seconds')\n",
        "print(total_duration/60,' in minutes')\n",
        "\n",
        "line_percentage=[0 for i in ideas]\n",
        "for i in range(len(ideas)) :\n",
        "  line_percentage[i]=(total_duration*len(ideas[i])/c)+0.5\n",
        "\n",
        "print(line_percentage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7qVOv1tYpq3",
        "outputId": "b37516f0-700d-4010-c379-eaf0fe2251a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37.0  in seconds\n",
            "0.6166666666666667  in minutes\n",
            "[8.102739726027398, 17.226027397260275, 13.17123287671233]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Animations"
      ],
      "metadata": {
        "id": "oL4Is76diAkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zoom_in(image_path, output_path, duration=3, fps=30, max_scale=1.15):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    # Define the codec and create VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Calculate the scale increment per frame\n",
        "    scale_increment = (max_scale - 1.0) / (duration * fps)\n",
        "\n",
        "    # Calculate the center of the image\n",
        "    center_x, center_y = width // 2, height // 2\n",
        "\n",
        "    # Create the zoom-out frames\n",
        "    current_scale = 1.0\n",
        "    for i in range(int(duration * fps)):\n",
        "        # Calculate the current scale for this frame\n",
        "        current_scale += scale_increment\n",
        "\n",
        "        # Calculate the translation to keep the center fixed\n",
        "        translation_x = center_x * (1 - current_scale)\n",
        "        translation_y = center_y * (1 - current_scale)\n",
        "\n",
        "        # Create a scaling and translation matrix to zoom out the image from the center\n",
        "        transformation_matrix = np.float32([[current_scale, 0, translation_x], [0, current_scale, translation_y]])\n",
        "\n",
        "        # Apply the transformation to the image\n",
        "        zoomed_in_image = cv2.warpAffine(image, transformation_matrix, (width, height))\n",
        "\n",
        "        # Write the frame to the video\n",
        "        out.write(zoomed_in_image)\n",
        "\n",
        "    # Release the video writer\n",
        "    out.release()\n",
        "\n",
        "\n",
        "def zoom_out(image_path, output_path, duration=3, fps=30, max_scale=1.15):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    # Define the codec and create VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Calculate the scale decrement per frame\n",
        "    scale_decrement = (max_scale - 1.0) / (duration * fps)\n",
        "\n",
        "    # Calculate the center of the image\n",
        "    center_x, center_y = width // 2, height // 2\n",
        "\n",
        "    # Create the zoom-in frames\n",
        "    current_scale = max_scale\n",
        "    for i in range(int(duration * fps)):\n",
        "        # Calculate the current scale for this frame\n",
        "        current_scale -= scale_decrement\n",
        "\n",
        "        # Calculate the translation to keep the center fixed\n",
        "        translation_x = center_x * (1 - current_scale)\n",
        "        translation_y = center_y * (1 - current_scale)\n",
        "\n",
        "        # Create a scaling and translation matrix to zoom in the image from the center\n",
        "        transformation_matrix = np.float32([[current_scale, 0, translation_x], [0, current_scale, translation_y]])\n",
        "\n",
        "        # Apply the transformation to the image\n",
        "        zoomed_out_image = cv2.warpAffine(image, transformation_matrix, (width, height))\n",
        "\n",
        "        # Write the frame to the video\n",
        "        out.write(zoomed_out_image)\n",
        "\n",
        "    # Release the video writer\n",
        "    out.release()\n",
        "\n",
        "def translate_left(image_path, output_path, duration=3, fps=30):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    # Calculate the width of the visible portion (90% of the image width)\n",
        "    visible_width = int(width * 0.9)\n",
        "\n",
        "    # Define the codec and create VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (visible_width, height))\n",
        "\n",
        "    # Calculate the total translation distance for the entire duration\n",
        "    total_translation_distance = width - visible_width\n",
        "\n",
        "    # Calculate the translation distance per second\n",
        "    translation_per_second = total_translation_distance / duration\n",
        "\n",
        "    # Create the translated frames\n",
        "    for i in range(int(duration * fps)):\n",
        "        # Calculate the translation distance for this frame\n",
        "        translation_distance = int(i / fps * translation_per_second)\n",
        "\n",
        "        # Create a translation matrix to shift the image to the left\n",
        "        translation_matrix = np.float32([[1, 0, -translation_distance], [0, 1, 0]])\n",
        "\n",
        "        # Apply the translation to the image\n",
        "        translated_image = cv2.warpAffine(image, translation_matrix, (visible_width, height))\n",
        "\n",
        "        # Write the frame to the video\n",
        "        out.write(translated_image)\n",
        "\n",
        "    # Release everything if job is finished\n",
        "    out.release()\n",
        "\n",
        "\n",
        "def translate_right(image_path, output_path, duration=3, fps=30):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    # Calculate the width of the visible portion (90% of the image width)\n",
        "    visible_width = int(width * 0.9)\n",
        "\n",
        "    # Define the codec and create VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (visible_width, height))\n",
        "\n",
        "    # Calculate the total translation distance for the entire duration\n",
        "    total_translation_distance = width - visible_width\n",
        "\n",
        "    # Calculate the translation distance per second\n",
        "    translation_per_second = total_translation_distance / duration\n",
        "\n",
        "    # Create the translated frames\n",
        "    frames = []\n",
        "    for i in range(int(duration * fps)):\n",
        "        # Calculate the translation distance for this frame\n",
        "        translation_distance = int(i / fps * translation_per_second)\n",
        "\n",
        "        # Create a translation matrix to shift the image to the left\n",
        "        translation_matrix = np.float32([[1, 0, -translation_distance], [0, 1, 0]])\n",
        "\n",
        "        # Apply the translation to the image\n",
        "        translated_image = cv2.warpAffine(image, translation_matrix, (visible_width, height))\n",
        "\n",
        "        # Append the frame to the frames list\n",
        "        frames.append(translated_image)\n",
        "\n",
        "    # Append the frames in reverse order to the output video to get the inverse effect\n",
        "    for frame in reversed(frames):\n",
        "        out.write(frame)\n",
        "\n",
        "    # Release everything if job is finished\n",
        "    out.release()"
      ],
      "metadata": {
        "id": "8HIMOxpPh3mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#video creation phase 1:"
      ],
      "metadata": {
        "id": "FgujKy-3iF0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to concatenate video clips into a single video using moviepy\n",
        "def concatenate_videos(video_paths, output_path, target_resolution=(1280,720)):\n",
        "    # Create VideoFileClip objects for each video and resize to the target resolution\n",
        "    video_clips = [VideoFileClip(video_path).resize(target_resolution) for video_path in video_paths]\n",
        "\n",
        "    # Concatenate the video clips\n",
        "    final_clip = concatenate_videoclips(video_clips, method=\"compose\")\n",
        "\n",
        "    # Write the final video to the output path\n",
        "    final_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n",
        "\n",
        "    # Close the video clips\n",
        "    for video_clip in video_clips:\n",
        "        video_clip.close()\n",
        "\n",
        "# Function to create a video with transitions between images\n",
        "def create_transition_video(image_paths, output_path, total_duration, fps=30, animations=None):\n",
        "    if not animations:\n",
        "        animations = [zoom_in, translate_left, zoom_out, translate_right]\n",
        "\n",
        "    # Create temporary video animations for each image\n",
        "    video_paths = []\n",
        "    i=0\n",
        "    for image_path in image_paths:\n",
        "      video_path=f'temp_video_for_image_n_{i}.mp4'\n",
        "      create_image_animation=random.choice(animations)\n",
        "      create_image_animation(image_path, video_path, total_duration / len(image_paths), fps)\n",
        "      video_paths.append(video_path)\n",
        "      i+=1\n",
        "\n",
        "    # Concatenate the temporary videos to create the final video\n",
        "    concatenate_videos(video_paths, output_path)\n",
        "\n",
        "    # Remove temporary video files\n",
        "    for video_path in video_paths:\n",
        "        os.remove(video_path)"
      ],
      "metadata": {
        "id": "7z8ElGtFiQmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_transition_video(image_paths, output_path, total_duration, fps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv1v1ruqiRgQ",
        "outputId": "16b3d6ae-810b-4098-96c4-09574195844e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video video.mp4.\n",
            "Moviepy - Writing video video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Video creation phase 2:"
      ],
      "metadata": {
        "id": "iiRgFXqTiYMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CV2"
      ],
      "metadata": {
        "id": "k50yLlhdvijr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Option 1"
      ],
      "metadata": {
        "id": "vYk9wK8pvnYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the video file\n",
        "cap = cv2.VideoCapture(output_path)\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "\n",
        "# Calculate the start and end frames for each subtitle\n",
        "start_frames = [int(sum(line_percentage[:i]) * fps) for i in range(len(line_percentage))]\n",
        "end_frames = [int(sum(line_percentage[:i+1]) * fps) for i in range(len(line_percentage))]\n",
        "\n",
        "# Set up the video writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(final_video, fourcc, fps, (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "# Add the subtitles to the video\n",
        "frame_num = 0\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Check if we need to add a subtitle to this frame\n",
        "    for i, (start, end) in enumerate(zip(start_frames, end_frames)):\n",
        "        if start <= frame_num < end:\n",
        "            # Add the subtitle to the frame\n",
        "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "            text = ideas[i]\n",
        "            textsize = cv2.getTextSize(text, font, 1, 2)[0]\n",
        "            textX = (frame.shape[1] - textsize[0]) // 2\n",
        "            textY = frame.shape[0] - 50\n",
        "            cv2.putText(frame, text, (textX, textY), font, 1, (255, 255, 255), 2)\n",
        "\n",
        "    # Write the frame to the output video\n",
        "    out.write(frame)\n",
        "    frame_num += 1\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "out.release()\n"
      ],
      "metadata": {
        "id": "1G9Gth7QhYyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Option 2"
      ],
      "metadata": {
        "id": "Og5YkJ7wvrel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import ImageFont, ImageDraw, Image\n",
        "\n",
        "# Load the video file\n",
        "cap = cv2.VideoCapture(output_path)\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Calculate the start and end frames for each subtitle\n",
        "start_frames = [int(sum(line_percentage[:i]) * fps) for i in range(len(line_percentage))]\n",
        "end_frames = [int(sum(line_percentage[:i+1]) * fps) for i in range(len(line_percentage))]\n",
        "\n",
        "# Set up the video writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(final_video, fourcc, fps, (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "# Add the subtitles to the video\n",
        "frame_num = 0\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert the frame from BGR to RGB color space\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Convert the frame to a PIL Image\n",
        "    pil_image = Image.fromarray(frame)\n",
        "\n",
        "    # Create a drawing context for the PIL Image\n",
        "    draw = ImageDraw.Draw(pil_image)\n",
        "\n",
        "    # Check if we need to add a subtitle to this frame\n",
        "    for i, (start, end) in enumerate(zip(start_frames, end_frames)):\n",
        "        if start <= frame_num < end:\n",
        "            # Add the subtitle to the frame\n",
        "            font_path = '/content/ReadexPro-Regular.ttf'\n",
        "            font_size = 32\n",
        "            font = ImageFont.truetype(font_path, font_size)\n",
        "            text = ideas[i]\n",
        "            textsize = font.getsize(text)\n",
        "            textX = 10\n",
        "            textY = frame.shape[0] - 50 - textsize[1]\n",
        "\n",
        "            # Add some padding between the text and its black background\n",
        "            padding = 10\n",
        "\n",
        "            # Add a black background for the subtitle\n",
        "            draw.rectangle((textX - padding, textY + textsize[1] + padding, textX + textsize[0] + padding, textY - padding), fill=(0, 0, 0))\n",
        "\n",
        "            # Put the subtitle text in color #FFC800\n",
        "            draw.text((textX, textY), text, fill=(255, 200, 0), font=font)\n",
        "\n",
        "    # Convert the PIL Image back to a NumPy array\n",
        "    frame = np.array(pil_image)\n",
        "\n",
        "    # Convert the frame back to BGR color space\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Write the frame to the output video\n",
        "    out.write(frame)\n",
        "    frame_num += 1\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "out.release()"
      ],
      "metadata": {
        "id": "zpFCdwjKmP7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4e2514c-9da4-431a-eaae-eb151b72d459"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:<ipython-input-6-b919ba98ed2c>:41: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
            "  textsize = font.getsize(text)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}